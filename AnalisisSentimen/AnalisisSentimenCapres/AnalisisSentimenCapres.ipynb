{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataCapres/anies.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['full_text', 'username', 'created_at']] # mengambil kolom full_text, username, dan created_at\n",
    "df # menampilkan dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kegunaannya untuk mengetahui jumlah baris dan kolom dari dataframe\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus data duplikat berdasarkan kolom full_text\n",
    "df = df.drop_duplicates(subset=['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengecek apakah masih ada data duplikat\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus baris yang memiliki nilai NaN atau null\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengecek apakah masih ada nilai NaN atau null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengecek apakah ada perubahan jumlah baris\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pembuatan function untuk melakukan pembersihan\n",
    "def clean_twitter_texr(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#\\w+', '', text) # menghapus hastag\n",
    "    text = re.sub(r'RT[\\s]+', '', text) # menghapus retweet\n",
    "    text = re.sub(r'https?://\\S+', '', text) # menghapus url/link\n",
    "\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # menghapus simbol (non-alfanumerik)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # menghapus spasi yang berlebihan\n",
    "\n",
    "    return text\n",
    "\n",
    "# menerapkan function untuk pembersihan teks\n",
    "df['full_text'] = df['full_text'].apply(clean_twitter_texr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengubah teks menjadi huruf kecil\n",
    "df['full_text'] = df['full_text'].str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengecek apakah ada perubahan\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 2. Preprocessing\n",
    "   - Normalisasi\n",
    "   - Stopword\n",
    "   - Tokenize\n",
    "   - Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi\n",
    "\n",
    "# ini adalah proses mengganti kata yang salah dengan kata yang benar\n",
    "# karena ini adalah bagian data anies, maka kita juga akan menghapus jika ada nama prabowo atau ganjar\n",
    "# norm adalah dictionary yang berisi kata yang salah dan kata yang benar\n",
    "norm = {\" yg \" : \" yang \", \n",
    "        \" ganjar \" : \" \", \n",
    "        \" prabowo \" : \" \", \n",
    "        \"pram \" : \" \", \n",
    "        \" ngga \" : \" tidak \", \n",
    "        \" nggak \" : \" tidak \", \n",
    "        \" ga \" : \" tidak \", \n",
    "        \" gak \" : \" tidak \", \n",
    "        \" vibes \" : \" suasana \", \n",
    "        \" text \" : \" teks \", \n",
    "        \" mantab \" : \" keren \", \n",
    "        \" end \" : \" selesai \", \n",
    "        \" kelen \" : \" kalian \", \n",
    "        \" jd \" : \" jadi \", \n",
    "        \" tuk \" : \" untuk \", \n",
    "        \" bangetdari \" : \" banget dari \", \n",
    "        \" disampaikam \" : \" disampaikan \", \n",
    "        \" kk \" : \" kakak \"}\n",
    "\n",
    "# membuat function normalisasi \n",
    "def normalisasi(str_text):\n",
    "    # perulangan ini bertujuan untuk mengganti kata yang salah dengan kata yang benar diatas\n",
    "    for i in norm:\n",
    "        str_text = str_text.replace(i, norm[i])\n",
    "    return str_text\n",
    "\n",
    "# menerapkan function\n",
    "df['full_text'] = df['full_text'].apply(lambda x: normalisasi(x))\n",
    "\n",
    "# mengecek apakah ada perubahan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "\n",
    "import Sastrawi\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "# ini akan membuang kata kata yang tidak memiliki bobot, contohnya kata di, yang, dengan, dan lain lain\n",
    "# disini saya tambahkan kata \"tidak\", karena saya merasa ini juga tidak memiliki bobot\n",
    "more_stop_words = [\"tidak\"]\n",
    "\n",
    "stop_words = StopWordRemoverFactory().get_stop_words()\n",
    "stop_words.extend(more_stop_words)\n",
    "\n",
    "new_array = ArrayDictionary(stop_words)\n",
    "stop_words_remover_new = StopWordRemover(new_array)\n",
    "\n",
    "def stopword(str_text):\n",
    "    str_text = stop_words_remover_new.remove(str_text)\n",
    "    return str_text\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(lambda X: stopword(X))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "# kegunaan tokenize ini adalah untuk memecah teks menjadi kata-kata\n",
    "tokenized = df['full_text'].apply(lambda x:x.split())\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "# # fungsinya untuk mengubah kata yang tadinya memiliki imbuhan menjadi kata dasar\n",
    "# # contoh : menipu menjadi tipu\n",
    "\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# def stemming(text_cleaning):\n",
    "#     factory = StemmerFactory()\n",
    "#     stemmer = factory.create_stemmer()\n",
    "#     do = []\n",
    "#     for w in text_cleaning:\n",
    "#         dt = stemmer.stem(w)\n",
    "#         do.append(dt)\n",
    "#     d_clean = []\n",
    "#     d_clean = ' '.join(do)\n",
    "#     print(d_clean)\n",
    "#     return d_clean\n",
    "\n",
    "# tokenized = tokenized.apply(stemming)\n",
    "\n",
    "# # karena proses ini lama, kita akan ngebackup data ke dalam csv, agar kedepannya tidak perlu melakukan proses ini lagi\n",
    "# # menyimpan data ke dalam csv \n",
    "\n",
    "# tokenized.to_csv('dataPreprocessing/PreprocessingAnies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memasukkan data hasil stemming ke dalam dataframe\n",
    "data = pd.read_csv('dataPreprocessing/PreprocessingAnies.csv', encoding='latin-1')\n",
    "data.head()\n",
    "\n",
    "#kenapa atributnya cuma 1 yaitu full_text? karena untuk proses pengolahan labeling ataupun analisis datanya kita hanya memerlukan atribut komentar dari masyarakat yaitu full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from translate import Translator\n",
    "\n",
    "# # fungsi ini berguna untuk mengubah teks dari bahasa indonesia menjadi bahasa inggris\n",
    "# def convert_eng(tweet):\n",
    "#     translator = Translator(to_lang=\"en\", from_lang=\"id\")\n",
    "#     translation = translator.translate(tweet)\n",
    "#     return translation\n",
    "\n",
    "# # bagian mengapply fungsi di atas ke dalam data\n",
    "# data['tweet_english'] = data['full_text'].apply(convert_eng)\n",
    "\n",
    "# # menyimpan/backup data ke dalam csv, karena proses ini lumayan lama, jadi kedepannya agar tidak perlu running bagian ini lagi\n",
    "# data.to_csv('dataTerjemahan/TerjemahanAnies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memasukkan data hasil terjemahan ke dalam dataframe\n",
    "data = pd.read_csv('dataTerjemahan/TerjemahanAnies.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tweet-preprocessor\n",
    "# %pip install textblob\n",
    "# %pip install wordcloud\n",
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "# nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = list(data['tweet_english'])\n",
    "polaritas = 0\n",
    "\n",
    "status = [] # untuk melihat status sentimen\n",
    "total_positif = total_negatif = total_netral = total = 0\n",
    "\n",
    "# proses labeling\n",
    "for i, tweet in enumerate(data_tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    polaritas = analysis.polarity\n",
    "\n",
    "    if analysis.sentiment.polarity > 0.0:\n",
    "        total_positif += 1\n",
    "        status.append('Positif')\n",
    "    elif analysis.sentiment.polarity == 0.0:\n",
    "        total_netral += 1\n",
    "        status.append('Netral')\n",
    "    else:\n",
    "        total_negatif += 1\n",
    "        status.append('Negatif')\n",
    "\n",
    "    total += 1\n",
    "\n",
    "print(f'Hasil Analisis Data:\\nPositif = {total_positif}\\nNetral = {total_netral}\\nNegatif = {total_negatif}')\n",
    "print(f'\\nTotal Data: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['klasifikasi'] = status\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# kita akan menampilkan visualisasi workcloud atau menampilkan beberapa data yang memiliki kuantitas yang tinggi\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "all_words = ' '.join([tweet for tweet in data['full_text']])\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=3000, \n",
    "    height=2000, \n",
    "    random_state=3, \n",
    "    background_color='black', \n",
    "    colormap='Blues_r',\n",
    "    collocations=False,\n",
    "    stopwords=STOPWORDS\n",
    ").generate(all_words)\n",
    "\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "labels = ['Positif', 'Negatif', 'Netral']\n",
    "counts = [total_positif, total_negatif, total_netral]\n",
    "\n",
    "def show_bar_chart(labels, counts, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(labels, counts, color=['#2394f7', '#f72323', '#fac343'])\n",
    "\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{count}', \n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax.set_xlabel('Sentimen')\n",
    "    ax.set_ylabel('Jumlah')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "show_bar_chart(labels, counts, \"Distribusi Sentimen Anies Baswedan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Klasifikasi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.drop('full_text', axis=1, inplace=False)\n",
    "dataset = [tuple(x) for x in dataset.to_records(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# inisialisasi sentiment\n",
    "set_positif = []\n",
    "set_negatif = []\n",
    "set_neutral = []\n",
    "\n",
    "# membagi data menjadi 3 bagian\n",
    "# jika sentimennya positif, maka akan masuk ke set_positif, dan seterusnya\n",
    "for n in dataset:\n",
    "    if n[1] == 'Positif':\n",
    "        set_positif.append(n)\n",
    "    elif n[1] == 'Negatif':\n",
    "        set_negatif.append(n)\n",
    "    else:\n",
    "        set_neutral.append(n)\n",
    "\n",
    "# membagi data menjadi 2 bagian\n",
    "set_positif = random.sample(set_positif, k=int(len(set_positif) / 2))\n",
    "set_negatif = random.sample(set_negatif, k=int(len(set_negatif) / 2))\n",
    "set_neutral = random.sample(set_neutral, k=int(len(set_neutral) / 2))\n",
    "\n",
    "# menggabungkan set_positif, set_negatif, dan set_neutral, dan menyimpannya pada variabel train\n",
    "train = set_positif + set_negatif + set_neutral\n",
    "\n",
    "train_set = []\n",
    "\n",
    "for n in train:\n",
    "    train_set.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.classify import NaiveBayesClassifier\n",
    "# from nltk.classify import accuracy\n",
    "\n",
    "# # Format data menjadi format yang sesuai untuk classifier nltk\n",
    "# train_data = [(dict([(word, True) for word in doc.split()]), category) for doc, category in train_set]\n",
    "\n",
    "# # Melatih model\n",
    "# classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# # Menghitung akurasi\n",
    "# test_data = [(dict([(word, True) for word in doc.split()]), category) for doc, category in dataset]\n",
    "# print(\"Akurasi Test: \", accuracy(classifier, test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nltk\n",
    "\n",
    "# # Ensure we have a clean, writable download path\n",
    "# nltk_data_path = os.path.join(os.path.expanduser('~'), 'nltk_data')\n",
    "# os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# # Explicitly set the data path\n",
    "# nltk.data.path = [nltk_data_path]\n",
    "\n",
    "# # Download multiple related NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# # Verify downloads\n",
    "# try:\n",
    "#     # Force a download with a specific download directory\n",
    "#     nltk.download('punkt', download_dir=nltk_data_path)\n",
    "#     nltk.download('punkt_tab', download_dir=nltk_data_path)\n",
    "    \n",
    "#     # List available packages\n",
    "#     print(\"Available NLTK data:\")\n",
    "#     for package in nltk.corpus.corpora.fileids():\n",
    "#         print(package)\n",
    "# except Exception as e:\n",
    "#     print(f\"Download error: {e}\")\n",
    "\n",
    "# # Try tokenization\n",
    "# try:\n",
    "#     from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "#     text = \"Hello world. This is a test sentence.\"\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     print(\"\\nTokenized sentences:\", sentences)\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nTokenization error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "cl = NaiveBayesClassifier(train_set)\n",
    "print(\"Akurasi Test: \", cl.accuracy(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling\n",
    "data_tweet = list(data['tweet_english'])\n",
    "polaritas = []\n",
    "status = []\n",
    "total_positif = total_negatif = total_neutral = total = 0\n",
    "\n",
    "for i, tweet in enumerate(data_tweet):\n",
    "    analysis = TextBlob(tweet, classifier=cl)\n",
    "    \n",
    "    if analysis.classify() == 'Positif':\n",
    "        total_positif += 1\n",
    "    elif analysis.classify() == 'Neutral':\n",
    "        total_neutral += 1\n",
    "    else:\n",
    "        total_negatif += 1\n",
    "\n",
    "    status.append(analysis.classify())\n",
    "    total += 1\n",
    "\n",
    "print(f'Hasil Analisis Data:\\nPositif = {total_positif}\\nNeutral = {total_neutral}\\nNegatif = {total_negatif}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = pd.DataFrame({\"Klasifikasi Bayes\": status})\n",
    "data['klasifikasi_bayes'] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set tema seaborn untuk tampilan yang lebih profesional\n",
    "sns.set_theme()\n",
    "\n",
    "labels = ['Positif', 'Negatif', 'Netral']\n",
    "counts = [total_positif, total_negatif, total_neutral]\n",
    "\n",
    "def show_bar_chart(labels, counts, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(labels, counts, color=['#2394f7', '#f72323', '#fac343'])\n",
    "\n",
    "    # Menambahkan keterangan presentase\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{count}', \n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    # Menambahkan grid\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Menambahkan label sumbu dan judul\n",
    "    ax.set_xlabel('Sentimen')\n",
    "    ax.set_ylabel('Jumlah')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "show_bar_chart(labels, counts, \"Distribusi Sentimen Ganjar Pranowo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = [tuple(x) for x in data.to_records(index=False)]\n",
    "\n",
    "for n in data_eval:\n",
    "    if len(n) >= 4:\n",
    "        if n[2] == n[3]:\n",
    "            print(f'Text: {n[0]}\\nClassifier: {n[2]}\\nClassifier Bayes: {n[3]}\\n')\n",
    "    else:\n",
    "        print(\"Tuple tidak memiliki cukup elemen.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
